{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy paste från lab0\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        print(f\"Device: {str(self.device).upper()}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 64 * 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(correct_predictions, total_predictions, all_targets, all_predictions, all_probs, display_cm = True):\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / total_predictions\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    auc = roc_auc_score(all_targets, [p[1] for p in all_probs])\n",
    "        \n",
    "    print(f\"\\rAccuracy    : {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision   : {precision*100:.2f}%\")\n",
    "    print(f\"Recall      : {recall*100:.2f}%\")\n",
    "    print(f\"F1 Score    : {f1*100:.2f}%\")\n",
    "    print(f\"AUC Score   : {auc*100:.2f}%\")\n",
    "    print(f\"Specificity : {specificity*100:.2f}%\")\n",
    "    if display_cm:\n",
    "        plt.figure(figsize=(4,4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Truth')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(nn, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler = None, print_seperate = False, patience=5) -> None:\n",
    "    nn.to(nn.device)\n",
    "\n",
    "    n = int(len(str(abs(num_epochs))))\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        nn.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        ind = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(nn.device), labels.to(nn.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            print(f\"\\rTraining.. Epoch [{epoch+1:>n}/{num_epochs:>n}], ({ind+1:>3}/{len(train_loader)})\", end=\"\")\n",
    "            ind += 1\n",
    "\n",
    "        nn.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ind = 0\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(nn.device), labels.to(nn.device)\n",
    "\n",
    "                outputs = nn(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                print(f\"\\rValidating Epoch [{epoch+1:>n}/{num_epochs:>n}], ({ind+1:>3}/{len(val_loader):>3})\", end=\"\")\n",
    "                ind += 1\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        if scheduler:\n",
    "            # reducelr... vill tydligen ha lossen exlipcitly \n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(avg_val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        print(f'\\r.......... Epoch [{epoch+1:>n}/{num_epochs:>n}], ({len(val_loader):>3}/{len(val_loader):>3}), Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy*100:.2f}%', end=\"\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\" Early stopping: {epochs_no_improve}/{patience}\", end=\"\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\nNo improvement in {epochs_no_improve} epochs, stopping early.\")\n",
    "                break\n",
    "        # Early stopping \n",
    "        if print_seperate:\n",
    "            print()\n",
    "    print()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def test_model(nn, test_loader, display_cm = True) -> None:\n",
    "    print(\"\\rTesting...\", end = \"\")\n",
    "    nn.to(nn.device)\n",
    "    nn.eval()\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(nn.device), labels.to(nn.device)\n",
    "            outputs = nn(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            # För F1, Rec, Pre, AUC:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Compute evalulation metrics\n",
    "    evaluation_metrics(correct_predictions, total_predictions, all_targets, all_predictions, all_probs, display_cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensemble_prediction(models: list[torch.nn.Module], inputs: torch.Tensor, scheme: str = \"Majority Vote\", weights = [0.1, 0.2, 0.3, 0.4]):\n",
    "    device = models[0].device\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        match scheme: \n",
    "            case \"Majority Vote\":\n",
    "                predictions = [model(inputs).max(1)[1] for model in models]\n",
    "                stacked_predictions = torch.stack(predictions, dim=0)\n",
    "                return torch.mode(stacked_predictions, dim=0)[0]\n",
    "            case \"Average\":\n",
    "                outputs = sum(model(inputs) for model in models) / len(models)\n",
    "                return outputs\n",
    "            case \"Weighted Average\":\n",
    "                if len(weights) != len(models):\n",
    "                    raise ValueError(\"Number of models and weights do not match.\")\n",
    "                weights = torch.tensor(weights, device=device)  \n",
    "                outputs = sum(weights[i] * model(inputs) for i, model in enumerate(models))\n",
    "                return outputs / weights.sum()\n",
    "            case \"Soft Voting\":\n",
    "                probabilities = [F.softmax(model(inputs), dim=1) for model in models]\n",
    "                mean_probabilities = sum(probabilities) / len(models)\n",
    "                return torch.argmax(mean_probabilities, dim=1)\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid scheme.\")\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_ensemble(models: list[torch.nn.Module], test_loader: DataLoader, scheme: str = \"Majority Vote\",  display_cm: bool = True):\n",
    "    print(f\"Scheme: {scheme}\")\n",
    "    device = models[0].device\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = ensemble_prediction(models, inputs, scheme)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            # För F1, Rec, Pre, AUC:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    evaluation_metrics(correct_predictions, total_predictions, all_targets, all_predictions, all_probs, display_cm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_directory = \"C:/Users/hampu/Documents/kurser/år3/d7047e/images\"\n",
    "local_directory = \"C:/Users/hampek/Documents/school/d7047e/images\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4815], std=[0.2221]) # <-- beräknat med compute_mean_std() mean=[0.4815], std=[0.2221]\n",
    "])\n",
    "\n",
    "full_dataset = ImageFolder(root=local_directory, transform=transform)\n",
    "\n",
    "# 70/15/15 split\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True,  num_workers = num_workers)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size = batch_size, shuffle=False, num_workers = num_workers)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size = batch_size, shuffle=False, num_workers = num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(): \n",
    "    # tar fram mean och std för hela datasettet, till transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    data = ImageFolder(root=local_directory, transform=transform)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    total_samples = 0\n",
    "    for data, _ in loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        total_samples += batch_samples\n",
    "\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CPU\n",
      ".......... Epoch [1/100], ( 28/ 28), Train Loss: 0.2521, Val Loss: 0.1531, Accuracy: 95.22%\n",
      ".......... Epoch [2/100], ( 28/ 28), Train Loss: 0.1304, Val Loss: 0.1126, Accuracy: 95.44%\n",
      ".......... Epoch [3/100], ( 28/ 28), Train Loss: 0.1102, Val Loss: 0.1380, Accuracy: 94.31% Early stopping: 1/5\n",
      ".......... Epoch [4/100], ( 28/ 28), Train Loss: 0.0888, Val Loss: 0.1687, Accuracy: 94.31% Early stopping: 2/5\n",
      ".......... Epoch [5/100], ( 28/ 28), Train Loss: 0.0770, Val Loss: 0.1536, Accuracy: 94.65% Early stopping: 3/5\n",
      ".......... Epoch [6/100], ( 28/ 28), Train Loss: 0.0565, Val Loss: 0.1344, Accuracy: 95.90% Early stopping: 4/5\n",
      ".......... Epoch [7/100], ( 28/ 28), Train Loss: 0.0501, Val Loss: 0.1434, Accuracy: 95.22% Early stopping: 5/5\n",
      "No improvement in 5 epochs, stopping early.\n",
      "\n",
      "Accuracy    : 95.11%\n",
      "Precision   : 97.98%\n",
      "Recall      : 95.47%\n",
      "F1 Score    : 96.71%\n",
      "AUC Score   : 98.56%\n",
      "Specificity : 94.01%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN()\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, print_seperate=True)\n",
    "test_model(model, test_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Accuracy    : 95.11%\n",
    "#Precision   : 97.98%\n",
    "#Recall      : 95.47%\n",
    "#F1 Score    : 96.71%\n",
    "#AUC Score   : 98.56%\n",
    "#Specificity : 94.01%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256630\n",
    "num_epochs = 30\n",
    "learning_rate = 0.0001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "### FEATURE EXTRACTION ###\n",
    "fe_resnet    = models.resnet18(pretrained=True)\n",
    "fe_googlenet = models.googlenet(pretrained=True)\n",
    "fe_densenet  = models.densenet121(pretrained=True)\n",
    "\n",
    "fe_resnet.device    = _device\n",
    "fe_googlenet.device = _device\n",
    "fe_densenet.device  = _device\n",
    "\n",
    "fe_resnet.fc           = torch.nn.Linear(fe_resnet.fc.in_features, 2)\n",
    "fe_googlenet.fc        = torch.nn.Linear(fe_googlenet.fc.in_features, 2)\n",
    "fe_densenet.classifier = torch.nn.Linear(fe_densenet.classifier.in_features, 2)\n",
    "\n",
    "for name, param in fe_resnet.named_parameters():\n",
    "    if \"fc\" not in name: \n",
    "        param.requires_grad = False\n",
    "for name, param in fe_googlenet.named_parameters():\n",
    "    if \"fc\" not in name:\n",
    "        param.requires_grad = False\n",
    "for name, param in fe_densenet.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_fe_resnet    = torch.optim.Adam(fe_resnet.parameters(), lr=learning_rate)\n",
    "optimizer_fe_googlenet = torch.optim.Adam(fe_googlenet.parameters(), lr=learning_rate)\n",
    "optimizer_fe_densenet  = torch.optim.Adam(fe_densenet.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler_fe_resnet    = ReduceLROnPlateau(optimizer_fe_resnet)\n",
    "scheduler_fe_googlenet = ReduceLROnPlateau(optimizer_fe_googlenet)\n",
    "scheduler_fe_densenet  = ReduceLROnPlateau(optimizer_fe_densenet)\n",
    "\n",
    "### FINE TUNING ###\n",
    "ft_resnet    = models.resnet18(pretrained=True)\n",
    "ft_googlenet = models.googlenet(pretrained=True)\n",
    "ft_densenet  = models.densenet121(pretrained=True)\n",
    "\n",
    "ft_resnet.device    = _device\n",
    "ft_googlenet.device = _device\n",
    "ft_densenet.device  = _device\n",
    "\n",
    "ft_resnet.fc           = torch.nn.Linear(fe_resnet.fc.in_features, 2)\n",
    "ft_googlenet.fc        = torch.nn.Linear(fe_googlenet.fc.in_features, 2)\n",
    "ft_densenet.classifier = torch.nn.Linear(fe_densenet.classifier.in_features, 2)\n",
    "\n",
    "optimizer_ft_resnet    = torch.optim.Adam(ft_resnet.parameters(), lr=learning_rate)\n",
    "optimizer_ft_googlenet = torch.optim.Adam(ft_googlenet.parameters(), lr=learning_rate)\n",
    "optimizer_ft_densenet  = torch.optim.Adam(ft_densenet.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler_ft_resnet    = ReduceLROnPlateau(optimizer_ft_resnet)\n",
    "scheduler_ft_googlenet = ReduceLROnPlateau(optimizer_ft_googlenet)\n",
    "scheduler_ft_densenet  = ReduceLROnPlateau(optimizer_ft_densenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction. . .\n",
      "Network: RESNET\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m nn_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNetwork: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnn_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_seperate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnn_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_TRAINED_FEATURE_EXTRACTION.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(nn, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler, print_seperate, patience)\u001b[0m\n\u001b[0;32m     10\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Feature extraction. . .\")\n",
    "feature_extraction_zip = zip([fe_resnet, fe_googlenet, fe_densenet],\n",
    "    [optimizer_fe_resnet, optimizer_fe_googlenet, optimizer_fe_densenet],\n",
    "    [scheduler_fe_resnet, scheduler_fe_googlenet, scheduler_fe_densenet])\n",
    "\n",
    "for model, optimizer, scheduler in feature_extraction_zip:\n",
    "    nn_name = model.__class__.__name__\n",
    "    print(f\"Network: {nn_name}\")\n",
    "    train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler, print_seperate=True, patience=3)\n",
    "\n",
    "    model_path = f\"{nn_name.upper()}_TRAINED_FEATURE_EXTRACTION.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved {nn_name} to {model_path}\")\n",
    "\n",
    "    test_model(model, test_loader, display_cm = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fine tuning. . .\")\n",
    "fine_tuning_zip = zip([ft_resnet, ft_googlenet, ft_densenet],\n",
    "    [optimizer_ft_resnet, optimizer_ft_googlenet, optimizer_ft_densenet],\n",
    "    [scheduler_ft_resnet, scheduler_ft_googlenet, scheduler_ft_densenet])\n",
    "for model, optimizer, scheduler in fine_tuning_zip:\n",
    "    nn_name = model.__class__.__name__\n",
    "    print(f\"Network: {nn_name}\")\n",
    "    train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler, print_seperate=True, patience=3)\n",
    "\n",
    "    model_path = f\"{nn_name.upper()}_TRAINED_FINE_TUNING.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved {nn_name} to {model_path}\")\n",
    "\n",
    "    test_model(model, test_loader, display_cm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class, model_file):\n",
    "    model = model_class(pretrained=False) \n",
    "    if model_class == models.densenet121:\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features)\n",
    "    else:\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()  \n",
    "    print(f\"Loaded {model.__class__.__name__}.\")\n",
    "    return model\n",
    "\n",
    "resnet    = load_model(models.resnet18, \"\")\n",
    "googlenet = load_model(models.googlenet, \"\")\n",
    "densenet  = load_model(models.densenet121, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Majority Vote\",\"Average\",\"Weighted Average\",\"Soft Voting\"\n",
    "test_ensemble([resnet, googlenet, densenet], test_loader, scheme = \"Majority Vote\", display_cm = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
