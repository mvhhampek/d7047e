{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy paste från lab0\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        print(f\"Device: {str(self.device).upper()}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 64 * 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(correct_predictions, total_predictions, all_targets, all_predictions, display_cm = True, filename=\"confusion_matrix\", path = \"images\"):\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / total_predictions\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    #auc = roc_auc_score(all_targets, [p[1] for p in all_probs])\n",
    "        \n",
    "    print(f\"\\rAccuracy    : {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision   : {precision*100:.2f}%\")\n",
    "    print(f\"Recall      : {recall*100:.2f}%\")\n",
    "    print(f\"F1 Score    : {f1*100:.2f}%\")\n",
    "    #print(f\"AUC Score   : {auc*100:.2f}%\")\n",
    "    print(f\"Specificity : {specificity*100:.2f}%\")\n",
    "    if display_cm:\n",
    "        plt.figure(figsize=(4,4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Truth')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig(f'{path}/{filename}.png')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "def compute_gradcam(model, inputs, target_layer):\n",
    "    model.eval()\n",
    "    gradients = []\n",
    "    activations = []\n",
    "\n",
    "    def save_gradients(module, grad_input, grad_output):\n",
    "        print(\"Saving gradients\")\n",
    "        gradients.append(grad_output[0])\n",
    "        print(\"Gradients:\", type(gradients), \"Length:\", len(gradients))\n",
    "\n",
    "    def save_activations(module, input, output):\n",
    "        print(\"Saving activations\")\n",
    "        activations.append(output)\n",
    "        print(\"Activations:\", type(activations), \"Length:\", len(activations))\n",
    "\n",
    "    target_layer.register_forward_hook(save_activations)\n",
    "    target_layer.register_backward_hook(save_gradients)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    model.zero_grad()\n",
    "    target_class = outputs.argmax(dim=1)\n",
    "    one_hot_output = torch.zeros_like(outputs)\n",
    "    one_hot_output.scatter_(1, target_class.unsqueeze(1), 1)\n",
    "    outputs.backward(gradient=one_hot_output)\n",
    "\n",
    "    if gradients and activations:\n",
    "        print(\"Processing gradcam\")\n",
    "        gradients_np = gradients[0].cpu().data.numpy()\n",
    "        activations_np = activations[0].cpu().data.numpy()\n",
    "\n",
    "        weights = np.mean(gradients_np, axis=(2, 3))[0]\n",
    "        gradcam = np.sum(weights[:, np.newaxis, np.newaxis] * activations_np[0], axis=0)\n",
    "        gradcam = np.maximum(gradcam, 0)\n",
    "        gradcam = cv2.resize(gradcam, (inputs.shape[2], inputs.shape[3]))\n",
    "        gradcam -= gradcam.min()\n",
    "        gradcam /= gradcam.max()\n",
    "        return gradcam\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(nn, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler = None, print_seperate = False, patience=5) -> None:\n",
    "    nn.to(nn.device)\n",
    "\n",
    "    n = int(len(str(abs(num_epochs))))\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        nn.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        ind = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(nn.device), labels.to(nn.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            print(f\"\\rTraining.. Epoch [{epoch+1:>n}/{num_epochs:>n}], ({ind+1:>3}/{len(train_loader)})\", end=\"\")\n",
    "            ind += 1\n",
    "\n",
    "        nn.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ind = 0\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(nn.device), labels.to(nn.device)\n",
    "\n",
    "                outputs = nn(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                print(f\"\\rValidating Epoch [{epoch+1:>n}/{num_epochs:>n}], ({ind+1:>3}/{len(val_loader):>3})\", end=\"\")\n",
    "                ind += 1\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        if scheduler:\n",
    "            # reducelr... vill tydligen ha lossen exlipcitly \n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(avg_val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        print(f'\\r.......... Epoch [{epoch+1:>n}/{num_epochs:>n}], ({len(val_loader):>3}/{len(val_loader):>3}), Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy*100:.2f}%', end=\"\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\" Early stopping: {epochs_no_improve}/{patience}\", end=\"\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\nNo improvement in {epochs_no_improve} epochs, stopping early.\")\n",
    "                break\n",
    "        # Early stopping \n",
    "        if print_seperate:\n",
    "            print()\n",
    "    print()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def test_model(nn, test_loader, display_cm = True, visualize_gradcam = False, target_layer_name = None) -> None:\n",
    "    print(\"\\rTesting...\", end = \"\")\n",
    "    nn.to(nn.device)\n",
    "    nn.eval()\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(nn.device), labels.to(nn.device)\n",
    "            outputs = nn(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            # För F1, Rec, Pre, AUC:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    if visualize_gradcam:\n",
    "        class_names = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "        inputs.requires_grad = True  # Ensure gradients are enabled for inputs\n",
    "        target_layer = dict([*nn.named_modules()])[target_layer_name]\n",
    "        gradcam = compute_gradcam(nn, inputs, target_layer)\n",
    "\n",
    "        img = inputs.cpu().data.numpy()[0].transpose(1, 2, 0)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * gradcam), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        cam = heatmap + np.float32(img)\n",
    "        cam = cam / np.max(cam)\n",
    "\n",
    "        plt.imshow(cam)\n",
    "        if class_names:\n",
    "            plt.title(f\"Label: {class_names[labels[0].item()]}\")\n",
    "        else:\n",
    "            plt.title(f\"Label: {labels[0].item()}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Compute evalulation metrics\n",
    "    evaluation_metrics(correct_predictions, total_predictions, all_targets, all_predictions, display_cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensemble_prediction(models: list[torch.nn.Module], inputs: torch.Tensor, scheme: str = \"Majority Vote\", weights = [0.2, 0.3, 0.5]):\n",
    "    device = models[0].device\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Collect all model predictions\n",
    "        predictions = [model(inputs) for model in models]\n",
    "\n",
    "    # Normalize weights for Weighted Vote\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(models)\n",
    "    else:\n",
    "        total_weight = sum(weights)\n",
    "        weights = [w / total_weight for w in weights]\n",
    "\n",
    "    if scheme == \"Majority Vote\":\n",
    "        # Each model votes with the class index it predicts\n",
    "        pred_labels = [torch.argmax(pred, dim=1) for pred in predictions]\n",
    "        stacked_labels = torch.stack(pred_labels, dim=0)\n",
    "        final_pred, _ = torch.mode(stacked_labels, dim=0)\n",
    "        return final_pred\n",
    "    elif scheme == \"Weighted Vote\":\n",
    "        # Apply softmax then weight the probabilities\n",
    "        weighted_predictions = [weights[i] * F.softmax(pred, dim=1) for i, pred in enumerate(predictions)]\n",
    "        final_pred = torch.sum(torch.stack(weighted_predictions, dim=0), dim=0)\n",
    "        return final_pred\n",
    "    elif scheme == \"Average\":\n",
    "        # Apply softmax (if predictions are logits), then average\n",
    "        avg_preds = torch.mean(torch.stack([F.softmax(pred, dim=1) for pred in predictions], dim=0), dim=0)\n",
    "        return avg_preds  # Returning probabilities for all classes\n",
    "    elif scheme == \"Sum\":\n",
    "        # Sum of logits (assuming logits are returned)\n",
    "        summed_preds = torch.sum(torch.stack(predictions, dim=0), dim=0)\n",
    "        return summed_preds\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ensemble scheme: {scheme}\")\n",
    "        \n",
    "        # match scheme: \n",
    "        #     case \"Majority Vote\":\n",
    "        #         predictions = [model(inputs).max(1)[1] for model in models]\n",
    "        #         stacked_predictions = torch.stack(predictions, dim=0)\n",
    "        #         return torch.mode(stacked_predictions, dim=0)[0]\n",
    "        #     case \"Average\":\n",
    "        #         outputs = sum(model(inputs) for model in models) / len(models)\n",
    "        #         return outputs\n",
    "        #     case \"Weighted Average\":\n",
    "        #         if len(weights) != len(models):\n",
    "        #             raise ValueError(\"Number of models and weights do not match.\")\n",
    "        #         weights = torch.tensor(weights, device=device)  \n",
    "        #         outputs = sum(weights[i] * model(inputs) for i, model in enumerate(models))\n",
    "        #         return outputs / weights.sum()\n",
    "        #     case \"Soft Voting\":\n",
    "        #         probabilities = [F.softmax(model(inputs), dim=1) for model in models]\n",
    "        #         mean_probabilities = sum(probabilities) / len(models)\n",
    "        #         return torch.argmax(mean_probabilities, dim=1)\n",
    "        #     case _:\n",
    "        #         raise ValueError(f\"Invalid scheme.\")\n",
    "                \n",
    "        #         with torch.noarg():\n",
    "        # predictions = [model(inputs) for model in models]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_ensemble(models: list[torch.nn.Module], test_loader: DataLoader, scheme: str = \"Majority Vote\",  cm_params = (True, \"confusion_matrix\", \"images\")):\n",
    "    print(f\"Scheme: {scheme}\")\n",
    "    device = models[0].device\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    #all_probs = []\n",
    "    #probs = None\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = ensemble_prediction(models, inputs, scheme)\n",
    "            #print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "            if scheme == \"Sum\":\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            elif scheme == \"Majority Vote\":\n",
    "                predicted = outputs\n",
    "            elif scheme in[\"Weighted Vote\", \"Average\"]:\n",
    "                predicted = torch.argmax(outputs, 1)  # outputs are probabilities here\n",
    "\n",
    "\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            #if probs is not None:\n",
    "                #all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "\n",
    "    evaluation_metrics(correct_predictions, total_predictions, all_targets, all_predictions, *cm_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory = \"C:/Users/hampu/Documents/kurser/år3/d7047e/images\"\n",
    "#local_directory = \"C:/Users/hampek/Documents/school/d7047e/images\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4815], std=[0.2221]) # <-- beräknat med compute_mean_std() mean=[0.4815], std=[0.2221]\n",
    "])\n",
    "\n",
    "full_dataset = ImageFolder(root=local_directory, transform=transform)\n",
    "\n",
    "# 80/10/10 split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_subset_size = train_size // 3\n",
    "remaining_size = train_size - 2 * train_subset_size # pga 1/3 = 0.333333... => jobbigt läge\n",
    "train_dataset1, train_dataset2, train_dataset3 = random_split(train_dataset, [train_subset_size, train_subset_size, remaining_size])\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True,  num_workers = num_workers)\n",
    "\n",
    "train_loader_resnet   = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "train_loader_googlenet = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "train_loader_densenet  = DataLoader(train_dataset3, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "val_loader   = DataLoader(val_dataset,   batch_size = batch_size, shuffle=False, num_workers = num_workers)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size = batch_size, shuffle=False, num_workers = num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(): \n",
    "    # tar fram mean och std för hela datasettet, till transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    data = ImageFolder(root=local_directory, transform=transform)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    total_samples = 0\n",
    "    for data, _ in loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        total_samples += batch_samples\n",
    "\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CPU\n",
      ".......... Epoch [1/100], ( 28/ 28), Train Loss: 0.2521, Val Loss: 0.1531, Accuracy: 95.22%\n",
      ".......... Epoch [2/100], ( 28/ 28), Train Loss: 0.1304, Val Loss: 0.1126, Accuracy: 95.44%\n",
      ".......... Epoch [3/100], ( 28/ 28), Train Loss: 0.1102, Val Loss: 0.1380, Accuracy: 94.31% Early stopping: 1/5\n",
      ".......... Epoch [4/100], ( 28/ 28), Train Loss: 0.0888, Val Loss: 0.1687, Accuracy: 94.31% Early stopping: 2/5\n",
      ".......... Epoch [5/100], ( 28/ 28), Train Loss: 0.0770, Val Loss: 0.1536, Accuracy: 94.65% Early stopping: 3/5\n",
      ".......... Epoch [6/100], ( 28/ 28), Train Loss: 0.0565, Val Loss: 0.1344, Accuracy: 95.90% Early stopping: 4/5\n",
      ".......... Epoch [7/100], ( 28/ 28), Train Loss: 0.0501, Val Loss: 0.1434, Accuracy: 95.22% Early stopping: 5/5\n",
      "No improvement in 5 epochs, stopping early.\n",
      "\n",
      "Accuracy    : 95.11%\n",
      "Precision   : 97.98%\n",
      "Recall      : 95.47%\n",
      "F1 Score    : 96.71%\n",
      "AUC Score   : 98.56%\n",
      "Specificity : 94.01%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN()\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, print_seperate=True)\n",
    "test_model(model, test_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Accuracy    : 95.11%\n",
    "#Precision   : 97.98%\n",
    "#Recall      : 95.47%\n",
    "#F1 Score    : 96.71%\n",
    "#AUC Score   : 98.56%\n",
    "#Specificity : 94.01%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\hampek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256630\n",
    "num_epochs = 30\n",
    "learning_rate = 0.0001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Interviewer : If you want to repeat this code again, how would you do that?\n",
    "#          Me : Copy paste\n",
    "\n",
    "### FEATURE EXTRACTION ###\n",
    "fe_resnet    = models.resnet18(pretrained=True)\n",
    "fe_googlenet = models.googlenet(pretrained=True)\n",
    "fe_densenet  = models.densenet121(pretrained=True)\n",
    "\n",
    "fe_resnet.device    = _device\n",
    "fe_googlenet.device = _device\n",
    "fe_densenet.device  = _device\n",
    "\n",
    "fe_resnet.fc           = torch.nn.Linear(fe_resnet.fc.in_features, 2)\n",
    "fe_googlenet.fc        = torch.nn.Linear(fe_googlenet.fc.in_features, 2)\n",
    "fe_densenet.classifier = torch.nn.Linear(fe_densenet.classifier.in_features, 2)\n",
    "\n",
    "for name, param in fe_resnet.named_parameters():\n",
    "    if \"fc\" not in name: \n",
    "        param.requires_grad = False\n",
    "for name, param in fe_googlenet.named_parameters():\n",
    "    if \"fc\" not in name:\n",
    "        param.requires_grad = False\n",
    "for name, param in fe_densenet.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_fe_resnet    = torch.optim.Adam(fe_resnet.parameters(), lr=learning_rate)\n",
    "optimizer_fe_googlenet = torch.optim.Adam(fe_googlenet.parameters(), lr=learning_rate)\n",
    "optimizer_fe_densenet  = torch.optim.Adam(fe_densenet.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler_fe_resnet    = ReduceLROnPlateau(optimizer_fe_resnet)\n",
    "scheduler_fe_googlenet = ReduceLROnPlateau(optimizer_fe_googlenet)\n",
    "scheduler_fe_densenet  = ReduceLROnPlateau(optimizer_fe_densenet)\n",
    "\n",
    "### FINE TUNING ###\n",
    "ft_resnet    = models.resnet18(pretrained=True)\n",
    "ft_googlenet = models.googlenet(pretrained=True)\n",
    "ft_densenet  = models.densenet121(pretrained=True)\n",
    "\n",
    "ft_resnet.device    = _device\n",
    "ft_googlenet.device = _device\n",
    "ft_densenet.device  = _device\n",
    "\n",
    "ft_resnet.fc           = torch.nn.Linear(ft_resnet.fc.in_features, 2)\n",
    "ft_googlenet.fc        = torch.nn.Linear(ft_googlenet.fc.in_features, 2)\n",
    "ft_densenet.classifier = torch.nn.Linear(ft_densenet.classifier.in_features, 2)\n",
    "\n",
    "optimizer_ft_resnet    = torch.optim.Adam(ft_resnet.parameters(), lr=learning_rate)\n",
    "optimizer_ft_googlenet = torch.optim.Adam(ft_googlenet.parameters(), lr=learning_rate)\n",
    "optimizer_ft_densenet  = torch.optim.Adam(ft_densenet.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler_ft_resnet    = ReduceLROnPlateau(optimizer_ft_resnet)\n",
    "scheduler_ft_googlenet = ReduceLROnPlateau(optimizer_ft_googlenet)\n",
    "scheduler_ft_densenet  = ReduceLROnPlateau(optimizer_ft_densenet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction. . .\n",
      "Network: ResNet\n",
      ".......... Epoch [1/30], ( 19/ 19), Train Loss: 0.5442, Val Loss: 0.4806, Accuracy: 72.65%\n",
      ".......... Epoch [2/30], ( 19/ 19), Train Loss: 0.4550, Val Loss: 0.4110, Accuracy: 78.12%\n",
      ".......... Epoch [3/30], ( 19/ 19), Train Loss: 0.4015, Val Loss: 0.3656, Accuracy: 82.91%\n",
      ".......... Epoch [4/30], ( 19/ 19), Train Loss: 0.3596, Val Loss: 0.3345, Accuracy: 85.47%\n",
      ".......... Epoch [5/30], ( 19/ 19), Train Loss: 0.3288, Val Loss: 0.3096, Accuracy: 87.01%\n",
      ".......... Epoch [6/30], ( 19/ 19), Train Loss: 0.3031, Val Loss: 0.2964, Accuracy: 88.03%\n",
      ".......... Epoch [7/30], ( 19/ 19), Train Loss: 0.2876, Val Loss: 0.2834, Accuracy: 88.21%\n",
      ".......... Epoch [8/30], ( 19/ 19), Train Loss: 0.2706, Val Loss: 0.2742, Accuracy: 87.69%\n",
      ".......... Epoch [9/30], ( 19/ 19), Train Loss: 0.2642, Val Loss: 0.2599, Accuracy: 88.89%\n",
      ".......... Epoch [10/30], ( 19/ 19), Train Loss: 0.2599, Val Loss: 0.2627, Accuracy: 88.55% Early stopping: 1/3\n",
      ".......... Epoch [11/30], ( 19/ 19), Train Loss: 0.2439, Val Loss: 0.2510, Accuracy: 89.57%\n",
      ".......... Epoch [12/30], ( 19/ 19), Train Loss: 0.2397, Val Loss: 0.2448, Accuracy: 89.40%\n",
      ".......... Epoch [13/30], ( 19/ 19), Train Loss: 0.2413, Val Loss: 0.2453, Accuracy: 89.91% Early stopping: 1/3\n",
      ".......... Epoch [14/30], ( 19/ 19), Train Loss: 0.2334, Val Loss: 0.2388, Accuracy: 89.40%\n",
      ".......... Epoch [15/30], ( 19/ 19), Train Loss: 0.2264, Val Loss: 0.2311, Accuracy: 90.26%\n",
      ".......... Epoch [16/30], ( 19/ 19), Train Loss: 0.2210, Val Loss: 0.2318, Accuracy: 89.91% Early stopping: 1/3\n",
      ".......... Epoch [17/30], ( 19/ 19), Train Loss: 0.2244, Val Loss: 0.2263, Accuracy: 90.94%\n",
      ".......... Epoch [18/30], ( 19/ 19), Train Loss: 0.2189, Val Loss: 0.2253, Accuracy: 90.26%\n",
      ".......... Epoch [19/30], ( 19/ 19), Train Loss: 0.2096, Val Loss: 0.2280, Accuracy: 89.74% Early stopping: 1/3\n",
      ".......... Epoch [20/30], ( 19/ 19), Train Loss: 0.2127, Val Loss: 0.2192, Accuracy: 90.09%\n",
      ".......... Epoch [21/30], ( 19/ 19), Train Loss: 0.2099, Val Loss: 0.2233, Accuracy: 89.74% Early stopping: 1/3\n",
      ".......... Epoch [22/30], ( 19/ 19), Train Loss: 0.2035, Val Loss: 0.2182, Accuracy: 90.43%\n",
      ".......... Epoch [23/30], ( 19/ 19), Train Loss: 0.1980, Val Loss: 0.2201, Accuracy: 89.91% Early stopping: 1/3\n",
      ".......... Epoch [24/30], ( 19/ 19), Train Loss: 0.2074, Val Loss: 0.2178, Accuracy: 90.94%\n",
      ".......... Epoch [25/30], ( 19/ 19), Train Loss: 0.2034, Val Loss: 0.2068, Accuracy: 90.26%\n",
      ".......... Epoch [26/30], ( 19/ 19), Train Loss: 0.2022, Val Loss: 0.2089, Accuracy: 90.94% Early stopping: 1/3\n",
      ".......... Epoch [27/30], ( 19/ 19), Train Loss: 0.1960, Val Loss: 0.2130, Accuracy: 89.91% Early stopping: 2/3\n",
      ".......... Epoch [28/30], ( 19/ 19), Train Loss: 0.1974, Val Loss: 0.2148, Accuracy: 90.09% Early stopping: 3/3\n",
      "No improvement in 3 epochs, stopping early.\n",
      "\n",
      "Saved ResNet to RESNET_TRAINED_FEATURE_EXTRACTION.pth\n",
      "Accuracy    : 91.48%\n",
      "Precision   : 93.06%\n",
      "Recall      : 95.63%\n",
      "F1 Score    : 94.33%\n",
      "Specificity : 79.61%\n",
      "Network: GoogLeNet\n",
      ".......... Epoch [1/30], ( 19/ 19), Train Loss: 0.5428, Val Loss: 0.5108, Accuracy: 72.31%\n",
      ".......... Epoch [2/30], ( 19/ 19), Train Loss: 0.4839, Val Loss: 0.4565, Accuracy: 72.65%\n",
      ".......... Epoch [3/30], ( 19/ 19), Train Loss: 0.4327, Val Loss: 0.4171, Accuracy: 75.56%\n",
      ".......... Epoch [4/30], ( 19/ 19), Train Loss: 0.3985, Val Loss: 0.3819, Accuracy: 80.34%\n",
      ".......... Epoch [5/30], ( 19/ 19), Train Loss: 0.3672, Val Loss: 0.3569, Accuracy: 85.30%\n",
      ".......... Epoch [6/30], ( 19/ 19), Train Loss: 0.3451, Val Loss: 0.3383, Accuracy: 85.64%\n",
      ".......... Epoch [7/30], ( 19/ 19), Train Loss: 0.3179, Val Loss: 0.3224, Accuracy: 87.01%\n",
      ".......... Epoch [8/30], ( 19/ 19), Train Loss: 0.3016, Val Loss: 0.3031, Accuracy: 89.06%\n",
      ".......... Epoch [9/30], ( 19/ 19), Train Loss: 0.2876, Val Loss: 0.2898, Accuracy: 89.57%\n",
      ".......... Epoch [10/30], ( 19/ 19), Train Loss: 0.2725, Val Loss: 0.2834, Accuracy: 89.74%\n",
      ".......... Epoch [11/30], ( 19/ 19), Train Loss: 0.2673, Val Loss: 0.2736, Accuracy: 89.74%\n",
      ".......... Epoch [12/30], ( 19/ 19), Train Loss: 0.2556, Val Loss: 0.2688, Accuracy: 90.60%\n",
      ".......... Epoch [13/30], ( 19/ 19), Train Loss: 0.2535, Val Loss: 0.2550, Accuracy: 89.74%\n",
      ".......... Epoch [14/30], ( 19/ 19), Train Loss: 0.2449, Val Loss: 0.2556, Accuracy: 89.74% Early stopping: 1/3\n",
      ".......... Epoch [15/30], ( 19/ 19), Train Loss: 0.2352, Val Loss: 0.2449, Accuracy: 90.77%\n",
      ".......... Epoch [16/30], ( 19/ 19), Train Loss: 0.2305, Val Loss: 0.2499, Accuracy: 89.91% Early stopping: 1/3\n",
      ".......... Epoch [17/30], ( 19/ 19), Train Loss: 0.2315, Val Loss: 0.2477, Accuracy: 90.43% Early stopping: 2/3\n",
      ".......... Epoch [18/30], ( 19/ 19), Train Loss: 0.2336, Val Loss: 0.2321, Accuracy: 90.77%\n",
      ".......... Epoch [19/30], ( 19/ 19), Train Loss: 0.2317, Val Loss: 0.2336, Accuracy: 90.77% Early stopping: 1/3\n",
      ".......... Epoch [20/30], ( 19/ 19), Train Loss: 0.2127, Val Loss: 0.2321, Accuracy: 90.26%\n",
      ".......... Epoch [21/30], ( 19/ 19), Train Loss: 0.2108, Val Loss: 0.2246, Accuracy: 91.62%\n",
      ".......... Epoch [22/30], ( 19/ 19), Train Loss: 0.2126, Val Loss: 0.2263, Accuracy: 91.28% Early stopping: 1/3\n",
      ".......... Epoch [23/30], ( 19/ 19), Train Loss: 0.2144, Val Loss: 0.2249, Accuracy: 90.26% Early stopping: 2/3\n",
      ".......... Epoch [24/30], ( 19/ 19), Train Loss: 0.2015, Val Loss: 0.2229, Accuracy: 91.45%\n",
      ".......... Epoch [25/30], ( 19/ 19), Train Loss: 0.2006, Val Loss: 0.2256, Accuracy: 90.77% Early stopping: 1/3\n",
      ".......... Epoch [26/30], ( 19/ 19), Train Loss: 0.2028, Val Loss: 0.2189, Accuracy: 90.43%\n",
      ".......... Epoch [27/30], ( 19/ 19), Train Loss: 0.2091, Val Loss: 0.2222, Accuracy: 89.91% Early stopping: 1/3\n",
      ".......... Epoch [28/30], ( 19/ 19), Train Loss: 0.1938, Val Loss: 0.2170, Accuracy: 90.43%\n",
      ".......... Epoch [29/30], ( 19/ 19), Train Loss: 0.1970, Val Loss: 0.2119, Accuracy: 91.28%\n",
      ".......... Epoch [30/30], ( 19/ 19), Train Loss: 0.1905, Val Loss: 0.2127, Accuracy: 91.79% Early stopping: 1/3\n",
      "\n",
      "Saved GoogLeNet to GOOGLENET_TRAINED_FEATURE_EXTRACTION.pth\n",
      "Accuracy    : 90.46%\n",
      "Precision   : 92.02%\n",
      "Recall      : 95.40%\n",
      "F1 Score    : 93.68%\n",
      "Specificity : 76.32%\n",
      "Network: DenseNet\n",
      ".......... Epoch [1/30], ( 19/ 19), Train Loss: 0.6023, Val Loss: 0.5164, Accuracy: 72.31%\n",
      ".......... Epoch [2/30], ( 19/ 19), Train Loss: 0.5112, Val Loss: 0.4754, Accuracy: 74.87%\n",
      ".......... Epoch [3/30], ( 19/ 19), Train Loss: 0.4688, Val Loss: 0.4299, Accuracy: 76.24%\n",
      ".......... Epoch [4/30], ( 19/ 19), Train Loss: 0.4343, Val Loss: 0.4052, Accuracy: 82.05%\n",
      ".......... Epoch [5/30], ( 19/ 19), Train Loss: 0.4036, Val Loss: 0.3749, Accuracy: 84.10%\n",
      ".......... Epoch [6/30], ( 19/ 19), Train Loss: 0.3734, Val Loss: 0.3527, Accuracy: 84.96%\n",
      ".......... Epoch [7/30], ( 19/ 19), Train Loss: 0.3515, Val Loss: 0.3356, Accuracy: 86.50%\n",
      ".......... Epoch [8/30], ( 19/ 19), Train Loss: 0.3346, Val Loss: 0.3191, Accuracy: 87.01%\n",
      ".......... Epoch [9/30], ( 19/ 19), Train Loss: 0.3186, Val Loss: 0.3061, Accuracy: 90.26%\n",
      ".......... Epoch [10/30], ( 19/ 19), Train Loss: 0.3032, Val Loss: 0.2962, Accuracy: 89.74%\n",
      ".......... Epoch [11/30], ( 19/ 19), Train Loss: 0.2854, Val Loss: 0.2899, Accuracy: 90.26%\n",
      ".......... Epoch [12/30], ( 19/ 19), Train Loss: 0.2798, Val Loss: 0.2786, Accuracy: 90.26%\n",
      ".......... Epoch [13/30], ( 19/ 19), Train Loss: 0.2703, Val Loss: 0.2706, Accuracy: 90.43%\n",
      ".......... Epoch [14/30], ( 19/ 19), Train Loss: 0.2573, Val Loss: 0.2636, Accuracy: 91.28%\n",
      ".......... Epoch [15/30], ( 19/ 19), Train Loss: 0.2492, Val Loss: 0.2528, Accuracy: 91.28%\n",
      ".......... Epoch [16/30], ( 19/ 19), Train Loss: 0.2437, Val Loss: 0.2511, Accuracy: 90.94%\n",
      ".......... Epoch [17/30], ( 19/ 19), Train Loss: 0.2400, Val Loss: 0.2428, Accuracy: 91.97%\n",
      ".......... Epoch [18/30], ( 19/ 19), Train Loss: 0.2328, Val Loss: 0.2414, Accuracy: 91.62%\n",
      ".......... Epoch [19/30], ( 19/ 19), Train Loss: 0.2239, Val Loss: 0.2393, Accuracy: 91.11%\n",
      ".......... Epoch [20/30], ( 19/ 19), Train Loss: 0.2205, Val Loss: 0.2326, Accuracy: 91.97%\n",
      ".......... Epoch [21/30], ( 19/ 19), Train Loss: 0.2171, Val Loss: 0.2300, Accuracy: 92.14%\n",
      ".......... Epoch [22/30], ( 19/ 19), Train Loss: 0.2123, Val Loss: 0.2240, Accuracy: 91.28%\n",
      ".......... Epoch [23/30], ( 19/ 19), Train Loss: 0.2125, Val Loss: 0.2216, Accuracy: 91.79%\n",
      ".......... Epoch [24/30], ( 19/ 19), Train Loss: 0.2074, Val Loss: 0.2235, Accuracy: 91.79% Early stopping: 1/3\n",
      ".......... Epoch [25/30], ( 19/ 19), Train Loss: 0.2012, Val Loss: 0.2197, Accuracy: 91.62%\n",
      ".......... Epoch [26/30], ( 19/ 19), Train Loss: 0.2001, Val Loss: 0.2132, Accuracy: 92.48%\n",
      ".......... Epoch [27/30], ( 19/ 19), Train Loss: 0.1914, Val Loss: 0.2153, Accuracy: 92.82% Early stopping: 1/3\n",
      ".......... Epoch [28/30], ( 19/ 19), Train Loss: 0.2006, Val Loss: 0.2117, Accuracy: 92.14%\n",
      ".......... Epoch [29/30], ( 19/ 19), Train Loss: 0.1974, Val Loss: 0.2106, Accuracy: 91.62%\n",
      ".......... Epoch [30/30], ( 19/ 19), Train Loss: 0.1842, Val Loss: 0.2085, Accuracy: 92.48%\n",
      "\n",
      "Saved DenseNet to DENSENET_TRAINED_FEATURE_EXTRACTION.pth\n",
      "Accuracy    : 91.99%\n",
      "Precision   : 93.69%\n",
      "Recall      : 95.63%\n",
      "F1 Score    : 94.65%\n",
      "Specificity : 81.58%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Feature extraction. . .\")\n",
    "feature_extraction_zip = zip(\n",
    "    [          fe_resnet,           fe_googlenet,           fe_densenet],\n",
    "    [optimizer_fe_resnet, optimizer_fe_googlenet, optimizer_fe_densenet],\n",
    "    [scheduler_fe_resnet, scheduler_fe_googlenet, scheduler_fe_densenet],\n",
    "    [train_loader_resnet, train_loader_googlenet, train_loader_densenet])\n",
    "\n",
    "for model, optimizer, scheduler, train_loader in feature_extraction_zip:\n",
    "    nn_name = model.__class__.__name__\n",
    "    print(f\"Network: {nn_name}\")\n",
    "    train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler, print_seperate=True, patience=3)\n",
    "\n",
    "    model_path = f\"{nn_name.upper()}_TRAINED_FEATURE_EXTRACTION.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved {nn_name} to {model_path}\")\n",
    "\n",
    "    test_model(model, test_loader, display_cm = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning. . .\n",
      "Network: ResNet\n",
      ".......... Epoch [1/30], ( 19/ 19), Train Loss: 0.2106, Val Loss: 0.1238, Accuracy: 95.73%\n",
      ".......... Epoch [2/30], ( 19/ 19), Train Loss: 0.0566, Val Loss: 0.1390, Accuracy: 95.56% Early stopping: 1/5\n",
      ".......... Epoch [3/30], ( 19/ 19), Train Loss: 0.0321, Val Loss: 0.1717, Accuracy: 95.38% Early stopping: 2/5\n",
      ".......... Epoch [4/30], ( 19/ 19), Train Loss: 0.0335, Val Loss: 0.1773, Accuracy: 96.07% Early stopping: 3/5\n",
      ".......... Epoch [5/30], ( 19/ 19), Train Loss: 0.0369, Val Loss: 0.1550, Accuracy: 96.07% Early stopping: 4/5\n",
      ".......... Epoch [6/30], ( 19/ 19), Train Loss: 0.0156, Val Loss: 0.1277, Accuracy: 96.07% Early stopping: 5/5\n",
      "No improvement in 5 epochs, stopping early.\n",
      "\n",
      "Saved ResNet to RESNET_TRAINED_FINE_TUNING.pth\n",
      "Accuracy    : 95.91%\n",
      "Precision   : 95.77%\n",
      "Recall      : 98.85%\n",
      "F1 Score    : 97.29%\n",
      "Specificity : 87.50%\n",
      "Network: GoogLeNet\n",
      ".......... Epoch [1/30], ( 19/ 19), Train Loss: 0.2367, Val Loss: 0.1400, Accuracy: 94.87%\n",
      ".......... Epoch [2/30], ( 19/ 19), Train Loss: 0.0918, Val Loss: 0.1175, Accuracy: 95.73%\n",
      ".......... Epoch [3/30], ( 19/ 19), Train Loss: 0.0536, Val Loss: 0.1087, Accuracy: 95.90%\n",
      ".......... Epoch [4/30], ( 19/ 19), Train Loss: 0.0347, Val Loss: 0.1072, Accuracy: 96.24%\n",
      ".......... Epoch [5/30], ( 19/ 19), Train Loss: 0.0292, Val Loss: 0.1213, Accuracy: 94.87% Early stopping: 1/5\n",
      ".......... Epoch [6/30], ( 19/ 19), Train Loss: 0.0224, Val Loss: 0.1281, Accuracy: 95.90% Early stopping: 2/5\n",
      ".......... Epoch [7/30], ( 19/ 19), Train Loss: 0.0164, Val Loss: 0.1321, Accuracy: 95.90% Early stopping: 3/5\n",
      ".......... Epoch [8/30], ( 19/ 19), Train Loss: 0.0081, Val Loss: 0.1190, Accuracy: 96.07% Early stopping: 4/5\n",
      ".......... Epoch [9/30], ( 19/ 19), Train Loss: 0.0148, Val Loss: 0.1682, Accuracy: 94.70% Early stopping: 5/5\n",
      "No improvement in 5 epochs, stopping early.\n",
      "\n",
      "Saved GoogLeNet to GOOGLENET_TRAINED_FINE_TUNING.pth\n",
      "Accuracy    : 94.21%\n",
      "Precision   : 96.30%\n",
      "Recall      : 95.86%\n",
      "F1 Score    : 96.08%\n",
      "Specificity : 89.47%\n",
      "Network: DenseNet\n",
      ".......... Epoch [1/30], ( 19/ 19), Train Loss: 0.2295, Val Loss: 0.1478, Accuracy: 95.04%\n",
      ".......... Epoch [2/30], ( 19/ 19), Train Loss: 0.0721, Val Loss: 0.1207, Accuracy: 95.56%\n",
      ".......... Epoch [3/30], ( 19/ 19), Train Loss: 0.0448, Val Loss: 0.1459, Accuracy: 94.87% Early stopping: 1/5\n",
      ".......... Epoch [4/30], ( 19/ 19), Train Loss: 0.0236, Val Loss: 0.1350, Accuracy: 95.38% Early stopping: 2/5\n",
      ".......... Epoch [5/30], ( 19/ 19), Train Loss: 0.0163, Val Loss: 0.1399, Accuracy: 95.21% Early stopping: 3/5\n",
      ".......... Epoch [6/30], ( 19/ 19), Train Loss: 0.0219, Val Loss: 0.1509, Accuracy: 95.21% Early stopping: 4/5\n",
      ".......... Epoch [7/30], ( 19/ 19), Train Loss: 0.0184, Val Loss: 0.3379, Accuracy: 88.89% Early stopping: 5/5\n",
      "No improvement in 5 epochs, stopping early.\n",
      "\n",
      "Saved DenseNet to DENSENET_TRAINED_FINE_TUNING.pth\n",
      "Accuracy    : 89.44%\n",
      "Precision   : 87.53%\n",
      "Recall      : 100.00%\n",
      "F1 Score    : 93.35%\n",
      "Specificity : 59.21%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fine tuning. . .\")\n",
    "fine_tuning_zip = zip(\n",
    "    [          ft_resnet,           ft_googlenet,           ft_densenet],\n",
    "    [optimizer_ft_resnet, optimizer_ft_googlenet, optimizer_ft_densenet],\n",
    "    [scheduler_ft_resnet, scheduler_ft_googlenet, scheduler_ft_densenet],\n",
    "    [train_loader_resnet, train_loader_googlenet, train_loader_densenet])\n",
    "\n",
    "for model, optimizer, scheduler, train_loader in fine_tuning_zip:\n",
    "    nn_name = model.__class__.__name__\n",
    "    print(f\"Network: {nn_name}\")\n",
    "    train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler, print_seperate=True, patience=5)\n",
    "\n",
    "    model_path = f\"{nn_name.upper()}_TRAINED_FINE_TUNING.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved {nn_name} to {model_path}\")\n",
    "\n",
    "    test_model(model, test_loader, display_cm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ResNet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hampu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GoogLeNet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hampu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DenseNet.\n",
      "Loaded ResNet.\n",
      "Loaded GoogLeNet.\n",
      "Loaded DenseNet.\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_class, model_file):\n",
    "    if model_class == models.googlenet:\n",
    "        model = model_class(pretrained=True)\n",
    "        # Adjusting the final layers of the main classifier and auxiliary classifiers\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "    elif model_class == models.densenet121:\n",
    "        model = model_class(pretrained=True)\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features, 2)\n",
    "    else:  # Assuming ResNet or similar\n",
    "        model = model_class(pretrained=True)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()  \n",
    "    print(f\"Loaded {model.__class__.__name__}.\")\n",
    "    return model\n",
    "\n",
    "_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ft_resnet    = load_model(models.resnet18, \"networks_same/RESNET_TRAINED_FINE_TUNING.pth\")\n",
    "ft_googlenet = load_model(models.googlenet, \"networks_same/GOOGLENET_TRAINED_FINE_TUNING.pth\")\n",
    "ft_densenet  = load_model(models.densenet121, \"networks_same/DENSENET_TRAINED_FINE_TUNING.pth\")\n",
    "fe_resnet    = load_model(models.resnet18, \"networks_same/RESNET_TRAINED_FEATURE_EXTRACTION.pth\")\n",
    "fe_googlenet = load_model(models.googlenet, \"networks_same/GOOGLENET_TRAINED_FEATURE_EXTRACTION.pth\")\n",
    "fe_densenet  = load_model(models.densenet121, \"networks_same/DENSENET_TRAINED_FEATURE_EXTRACTION.pth\")\n",
    "\n",
    "fe_resnet.device    = _device\n",
    "fe_googlenet.device = _device\n",
    "fe_densenet.device  = _device\n",
    "ft_resnet.device    = _device\n",
    "ft_googlenet.device = _device\n",
    "ft_densenet.device  = _device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE TUNING\n",
      "Scheme: Majority Vote\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\"Majority Vote\",\"Average\",\"Weighted Vote\",\"Sum\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINE TUNING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtest_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mft_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_googlenet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_densenet\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMajority Vote\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFT_CM_MAJORITY_VOTE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m test_ensemble([ft_resnet, ft_googlenet, ft_densenet], test_loader, scheme \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeighted Vote\u001b[39m\u001b[38;5;124m\"\u001b[39m, cm_params\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFT_CM_WEIGTHED_VOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m test_ensemble([ft_resnet, ft_googlenet, ft_densenet], test_loader, scheme \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage\u001b[39m\u001b[38;5;124m\"\u001b[39m, cm_params\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFT_CM_AVERAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[58], line 83\u001b[0m, in \u001b[0;36mtest_ensemble\u001b[1;34m(models, test_loader, scheme, cm_params)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     82\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 83\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m#print(\"Output shape:\", outputs.shape)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m, in \u001b[0;36mensemble_prediction\u001b[1;34m(models, inputs, scheme, weights)\u001b[0m\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Collect all model predictions\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Normalize weights for Weighted Vote\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hampu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hampu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hampu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hampu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hampu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hampu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1595\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1593\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1595\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[1;32mIn[45], line 36\u001b[0m, in \u001b[0;36mcompute_gradcam.<locals>.save_activations\u001b[1;34m(module, input, output)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_activations\u001b[39m(module, \u001b[38;5;28minput\u001b[39m, output):\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(output)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "#\"Majority Vote\",\"Average\",\"Weighted Vote\",\"Sum\"\n",
    "print(\"FINE TUNING\")\n",
    "test_ensemble([ft_resnet, ft_googlenet, ft_densenet], test_loader, scheme = \"Majority Vote\", cm_params=(True, \"FT_CM_MAJORITY_VOTE\", \"images\"))\n",
    "test_ensemble([ft_resnet, ft_googlenet, ft_densenet], test_loader, scheme = \"Weighted Vote\", cm_params=(True, \"FT_CM_WEIGTHED_VOTE\", \"images\"))\n",
    "test_ensemble([ft_resnet, ft_googlenet, ft_densenet], test_loader, scheme = \"Average\", cm_params=(True, \"FT_CM_AVERAGE\", \"images\"))\n",
    "test_ensemble([ft_resnet, ft_googlenet, ft_densenet], test_loader, scheme = \"Sum\", cm_params=(True, \"FT_CM_SUM\", \"images\"))\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "test_ensemble([fe_resnet, fe_googlenet, fe_densenet], test_loader, scheme = \"Majority Vote\", cm_params=(True, \"FE_CM_MAJORITY_VOTE\", \"images\"))\n",
    "test_ensemble([fe_resnet, fe_googlenet, fe_densenet], test_loader, scheme = \"Weighted Vote\", cm_params=(True, \"FE_CM_WEIGHTED_VOTE\", \"images\"))\n",
    "test_ensemble([fe_resnet, fe_googlenet, fe_densenet], test_loader, scheme = \"Average\", cm_params=(True, \"FE_CM_AVERAGE\", \"images\"))\n",
    "test_ensemble([fe_resnet, fe_googlenet, fe_densenet], test_loader, scheme = \"Sum\", cm_params=(True, \"FE_CM_SUM\", \"images\"))\n",
    "#test_model(ft_resnet,    test_loader, display_cm = True, visualize_gradcam=True, target_layer_name=\"layer4\")\n",
    "#test_model(ft_googlenet, test_loader, display_cm = True, visualize_gradcam=True, target_layer_name=\"inception5b\")\n",
    "#test_model(ft_densenet,  test_loader, display_cm = True, visualize_gradcam=True, target_layer_name=\"features.denseblock4.denselayer16\")\n",
    "# test_model(fe_resnet, test_loader, display_cm = True)\n",
    "# test_model(fe_googlenet, test_loader, display_cm = True)\n",
    "# test_model(fe_densenet, test_loader, display_cm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 predictions: [1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1]\n",
      "Model 1 predictions: [1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1]\n",
      "Model 2 predictions: [1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def check_model_predictions(models, test_loader):\n",
    "    device = next(models[0].parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            for i, model in enumerate(models):\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                print(f\"Model {i} predictions: {predicted.cpu().numpy()}\")\n",
    "            break  # Check only the first batch for brevity\n",
    "\n",
    "# Example usage\n",
    "check_model_predictions([fe_resnet, fe_googlenet, fe_densenet], test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561\n",
      "1561\n",
      "1562\n",
      "585\n",
      "587\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset1))\n",
    "print(len(train_dataset2))\n",
    "print(len(train_dataset3))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
